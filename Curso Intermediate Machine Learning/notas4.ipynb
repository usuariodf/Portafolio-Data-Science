{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":38454,"sourceType":"datasetVersion","datasetId":2709}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"En esta clase, aprenderá a utilizar la validación cruzada para obtener mejores medidas del rendimiento del modelo.","metadata":{}},{"cell_type":"markdown","source":"### Introducción\n\nEl aprendizaje automático es un proceso iterativo.\n\nTendrá que elegir qué variables predictivas utilizar, qué tipos de modelos utilizar, qué argumentos proporcionar a esos modelos, etc. Hasta ahora, ha tomado estas decisiones basándose en datos midiendo la calidad del modelo con una validación (o reserva) configurado.\n\nPero este enfoque tiene algunos inconvenientes. Para ver esto, imagina que tienes un conjunto de datos con 5000 filas. Normalmente mantendrá aproximadamente el 20% de los datos como conjunto de datos de validación, o 1000 filas. Pero esto deja cierta posibilidad aleatoria a la hora de determinar las puntuaciones del modelo. Es decir, un modelo podría funcionar bien en un conjunto de 1000 filas, incluso si sería inexacto en 1000 filas diferentes.\n\nEn un caso extremo, podría imaginarse tener solo 1 fila de datos en el conjunto de validación. Si comparas modelos alternativos, cuál hace las mejores predicciones sobre un único punto de datos será principalmente una cuestión de suerte.\n\nEn general, cuanto mayor sea el conjunto de validación, menos aleatoriedad (también conocida como \"ruido\") habrá en nuestra medida de la calidad del modelo y más confiable será. Desafortunadamente, solo podemos obtener un conjunto de validación grande eliminando filas de nuestros datos de entrenamiento, ¡y conjuntos de datos de entrenamiento más pequeños significan peores modelos!","metadata":{}},{"cell_type":"markdown","source":"¿Qué es la validación cruzada?\n\nEn la validación cruzada, ejecutamos nuestro proceso de modelado en diferentes subconjuntos de datos para obtener múltiples medidas de la calidad del modelo.\n\nPor ejemplo, podríamos comenzar dividiendo los datos en 5 partes, cada una de las cuales representa el 20% del conjunto de datos completo. En este caso decimos que hemos dividido los datos en 5 \"pliegues\".","metadata":{}},{"cell_type":"markdown","source":"Luego, realizamos un experimento para cada pliegue:\n\n* En el Experimento 1, utilizamos el primer pliegue como conjunto de validación (o reserva) y todo lo demás como datos de entrenamiento. Esto nos da una medida de la calidad del modelo basada en un conjunto de reservas del 20%.\n\n* En el Experimento 2, guardamos datos del segundo pliegue (y usamos todo excepto el segundo pliegue para entrenar el modelo). Luego, el conjunto de reservas se utiliza para obtener una segunda estimación de la calidad del modelo.\n\n* Repetimos este proceso, usando cada pliegue una vez como conjunto de reserva. En conjunto, el 100 % de los datos se utiliza como reserva en algún momento, y terminamos con una medida de la calidad del modelo que se basa en todas las filas del conjunto de datos (incluso si no usamos todas las filas simultáneamente) .","metadata":{}},{"cell_type":"markdown","source":"### Ejemplo\n\nTrabajaremos con los mismos datos que en el tutorial anterior. Cargamos los datos de entrada en X y los datos de salida en y.","metadata":{}},{"cell_type":"markdown","source":"### ¿Cuándo debería utilizar la validación cruzada?\n\nLa validación cruzada brinda una medida más precisa de la calidad del modelo, lo cual es especialmente importante si se toman muchas decisiones de modelado. Sin embargo, puede tardar más en ejecutarse porque estima varios modelos (uno para cada pliegue).\n\nEntonces, dadas estas compensaciones, ¿cuándo debería utilizar cada enfoque?\n\nPara conjuntos de datos pequeños, donde la carga computacional adicional no es gran cosa, debe ejecutar una validación cruzada.\nPara conjuntos de datos más grandes, un único conjunto de validación es suficiente. Su código se ejecutará más rápido y es posible que tenga suficientes datos para que no sea necesario reutilizar algunos de ellos para reservarlos.\nNo existe un umbral sencillo para determinar qué constituye un conjunto de datos grande o pequeño. Pero si su modelo tarda un par de minutos o menos en ejecutarse, probablemente valga la pena cambiar a la validación cruzada.\n\nAlternativamente, puede ejecutar una validación cruzada y ver si las puntuaciones de cada experimento parecen cercanas. Si cada experimento arroja los mismos resultados, probablemente un único conjunto de validación sea suficiente.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Read the data\ndata = pd.read_csv('/kaggle/input/melbourne-housing-snapshot/melb_data.csv')\n\n# Select subset of predictors\ncols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = data[cols_to_use]\n\n# Select target\ny = data.Price","metadata":{"execution":{"iopub.status.busy":"2024-03-21T21:37:29.586733Z","iopub.execute_input":"2024-03-21T21:37:29.587308Z","iopub.status.idle":"2024-03-21T21:37:31.199230Z","shell.execute_reply.started":"2024-03-21T21:37:29.587251Z","shell.execute_reply":"2024-03-21T21:37:31.197721Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Luego, definimos una canalización que utiliza un imputador para completar los valores faltantes y un modelo de bosque aleatorio para hacer predicciones.\n\nSi bien es posible realizar una validación cruzada sin canalizaciones, ¡es bastante difícil! El uso de una canalización hará que el código sea notablemente sencillo.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nmy_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n                              ('model', RandomForestRegressor(n_estimators=50,\n                                                              random_state=0))\n                             ])","metadata":{"execution":{"iopub.status.busy":"2024-03-21T21:42:46.506321Z","iopub.execute_input":"2024-03-21T21:42:46.507025Z","iopub.status.idle":"2024-03-21T21:42:46.514620Z","shell.execute_reply.started":"2024-03-21T21:42:46.506972Z","shell.execute_reply":"2024-03-21T21:42:46.513297Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Obtenemos las puntuaciones de validación cruzada con la función cross_val_score() de scikit-learn. Establecemos el número de pliegues con el parámetro cv.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"MAE scores:\\n\", scores)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T21:42:49.272670Z","iopub.execute_input":"2024-03-21T21:42:49.273175Z","iopub.status.idle":"2024-03-21T21:42:57.435449Z","shell.execute_reply.started":"2024-03-21T21:42:49.273137Z","shell.execute_reply":"2024-03-21T21:42:57.434182Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"MAE scores:\n [301628.7893587  303164.4782723  287298.331666   236061.84754543\n 260383.45111427]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"El parámetro de puntuación elige una medida de la calidad del modelo para informar: en este caso, elegimos el error absoluto medio negativo (MAE). Los documentos de scikit-learn muestran una lista de opciones.\n\nEs un poco sorprendente que especifiquemos MAE negativo. Scikit-learn tiene una convención en la que todas las métricas están definidas, por lo que un número alto es mejor. El uso de negativos aquí les permite ser consistentes con esa convención, aunque el MAE negativo es casi inaudito en otros lugares.\n\nNormalmente queremos una medida única de la calidad del modelo para comparar modelos alternativos. Entonces tomamos el promedio de los experimentos.","metadata":{}},{"cell_type":"code","source":"print(\"Average MAE score (across experiments):\")\nprint(scores.mean())","metadata":{"execution":{"iopub.status.busy":"2024-03-21T21:43:06.077260Z","iopub.execute_input":"2024-03-21T21:43:06.078798Z","iopub.status.idle":"2024-03-21T21:43:06.085386Z","shell.execute_reply.started":"2024-03-21T21:43:06.078734Z","shell.execute_reply":"2024-03-21T21:43:06.084079Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Average MAE score (across experiments):\n277707.3795913405\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Conclusión\n\nEl uso de la validación cruzada produce una medida mucho mejor de la calidad del modelo, con el beneficio adicional de limpiar nuestro código: tenga en cuenta que ya no necesitamos realizar un seguimiento de los conjuntos de capacitación y validación por separado. Entonces, especialmente para conjuntos de datos pequeños, ¡es una buena mejora!","metadata":{}}]}