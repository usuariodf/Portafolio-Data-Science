{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":38454,"sourceType":"datasetVersion","datasetId":2709}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"En esta clase, aprenderá cómo utilizar(pipelines) canalizaciones para limpiar su código de modelado.","metadata":{}},{"cell_type":"markdown","source":"# Introducción\n\nLas canalizaciones son una forma sencilla de mantener organizado el código de modelado y preprocesamiento de datos. Específicamente, una canalización agrupa pasos de preprocesamiento y modelado para que pueda utilizar todo el paquete como si fuera un solo paso.\n\nMuchos científicos de datos combinan modelos sin canalizaciones, pero las canalizaciones tienen algunos beneficios importantes. Estos incluyen:\n\n1. Código más limpio: la contabilidad de los datos en cada paso del preprocesamiento puede resultar complicada. Con una canalización, no necesitará realizar un seguimiento manual de sus datos de capacitación y validación en cada paso.\n\n2. Menos errores: hay menos oportunidades de aplicar mal un paso u olvidar un paso de preprocesamiento.\n\n3. Más fácil de producir: puede resultar sorprendentemente difícil hacer la transición de un modelo de un prototipo a algo que se pueda implementar a escala. No entraremos aquí en las muchas preocupaciones relacionadas, pero los oleoductos pueden ayudar.\n\n4. Más opciones para la validación del modelo: verá un ejemplo en el siguiente tutorial, que cubre la validación cruzada.","metadata":{}},{"cell_type":"markdown","source":"# Ejemplo\n\nNo nos centraremos en el paso de carga de datos. En cambio, puedes imaginar que estás en un punto en el que ya tienes los datos de entrenamiento y validación en X_train, X_valid, y_train e y_valid.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\ndata = pd.read_csv('/kaggle/input/melbourne-housing-snapshot/melb_data.csv')\n\n# Separate target from predictors\ny = data.Price\nX = data.drop(['Price'], axis=1)\n\n# Divide data into training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:49:09.331080Z","iopub.execute_input":"2024-03-21T17:49:09.332104Z","iopub.status.idle":"2024-03-21T17:49:12.479088Z","shell.execute_reply.started":"2024-03-21T17:49:09.332057Z","shell.execute_reply":"2024-03-21T17:49:12.477790Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Echamos un vistazo a los datos de entrenamiento con el método head() a continuación. Observe que los datos contienen datos categóricos y columnas con valores faltantes. ¡Con una tubería, es fácil lidiar con ambos!","metadata":{}},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:50:05.983603Z","iopub.execute_input":"2024-03-21T17:50:05.984072Z","iopub.status.idle":"2024-03-21T17:50:06.020031Z","shell.execute_reply.started":"2024-03-21T17:50:05.984039Z","shell.execute_reply":"2024-03-21T17:50:06.018485Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"      Type Method             Regionname  Rooms  Distance  Postcode  Bedroom2  \\\n12167    u      S  Southern Metropolitan      1       5.0    3182.0       1.0   \n6524     h     SA   Western Metropolitan      2       8.0    3016.0       2.0   \n8413     h      S   Western Metropolitan      3      12.6    3020.0       3.0   \n2919     u     SP  Northern Metropolitan      3      13.0    3046.0       3.0   \n6043     h      S   Western Metropolitan      3      13.3    3020.0       3.0   \n\n       Bathroom  Car  Landsize  BuildingArea  YearBuilt  Lattitude  \\\n12167       1.0  1.0       0.0           NaN     1940.0  -37.85984   \n6524        2.0  1.0     193.0           NaN        NaN  -37.85800   \n8413        1.0  1.0     555.0           NaN        NaN  -37.79880   \n2919        1.0  1.0     265.0           NaN     1995.0  -37.70830   \n6043        1.0  2.0     673.0         673.0     1970.0  -37.76230   \n\n       Longtitude  Propertycount  \n12167    144.9867        13240.0  \n6524     144.9005         6380.0  \n8413     144.8220         3755.0  \n2919     144.9158         8870.0  \n6043     144.8272         4217.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Method</th>\n      <th>Regionname</th>\n      <th>Rooms</th>\n      <th>Distance</th>\n      <th>Postcode</th>\n      <th>Bedroom2</th>\n      <th>Bathroom</th>\n      <th>Car</th>\n      <th>Landsize</th>\n      <th>BuildingArea</th>\n      <th>YearBuilt</th>\n      <th>Lattitude</th>\n      <th>Longtitude</th>\n      <th>Propertycount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12167</th>\n      <td>u</td>\n      <td>S</td>\n      <td>Southern Metropolitan</td>\n      <td>1</td>\n      <td>5.0</td>\n      <td>3182.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>1940.0</td>\n      <td>-37.85984</td>\n      <td>144.9867</td>\n      <td>13240.0</td>\n    </tr>\n    <tr>\n      <th>6524</th>\n      <td>h</td>\n      <td>SA</td>\n      <td>Western Metropolitan</td>\n      <td>2</td>\n      <td>8.0</td>\n      <td>3016.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>193.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-37.85800</td>\n      <td>144.9005</td>\n      <td>6380.0</td>\n    </tr>\n    <tr>\n      <th>8413</th>\n      <td>h</td>\n      <td>S</td>\n      <td>Western Metropolitan</td>\n      <td>3</td>\n      <td>12.6</td>\n      <td>3020.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>555.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-37.79880</td>\n      <td>144.8220</td>\n      <td>3755.0</td>\n    </tr>\n    <tr>\n      <th>2919</th>\n      <td>u</td>\n      <td>SP</td>\n      <td>Northern Metropolitan</td>\n      <td>3</td>\n      <td>13.0</td>\n      <td>3046.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>265.0</td>\n      <td>NaN</td>\n      <td>1995.0</td>\n      <td>-37.70830</td>\n      <td>144.9158</td>\n      <td>8870.0</td>\n    </tr>\n    <tr>\n      <th>6043</th>\n      <td>h</td>\n      <td>S</td>\n      <td>Western Metropolitan</td>\n      <td>3</td>\n      <td>13.3</td>\n      <td>3020.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>673.0</td>\n      <td>673.0</td>\n      <td>1970.0</td>\n      <td>-37.76230</td>\n      <td>144.8272</td>\n      <td>4217.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Construimos el pipeline completo en tres pasos.","metadata":{}},{"cell_type":"markdown","source":"### Paso 1: Definir los pasos de preprocesamiento\n\nDe manera similar a cómo una canalización agrupa los pasos de preprocesamiento y modelado, usamos la clase ColumnTransformer para agrupar diferentes pasos de preprocesamiento. El siguiente código:\n\n* imputa valores faltantes en datos numéricos, y\n\n* imputa valores faltantes y aplica una codificación one-hot a datos categóricos.","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:53:18.982613Z","iopub.execute_input":"2024-03-21T17:53:18.983189Z","iopub.status.idle":"2024-03-21T17:53:19.319770Z","shell.execute_reply.started":"2024-03-21T17:53:18.983134Z","shell.execute_reply":"2024-03-21T17:53:19.318318Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Paso 2: Definir el modelo\n\nA continuación, definimos un modelo de bosque aleatorio con la conocida clase RandomForestRegressor.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:55:05.866144Z","iopub.execute_input":"2024-03-21T17:55:05.866631Z","iopub.status.idle":"2024-03-21T17:55:05.874253Z","shell.execute_reply.started":"2024-03-21T17:55:05.866595Z","shell.execute_reply":"2024-03-21T17:55:05.872767Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Paso 3: Crear y evaluar la canalización\n\nFinalmente, usamos la clase Pipeline para definir una canalización que agrupa los pasos de preprocesamiento y modelado. Hay algunas cosas importantes a tener en cuenta:\n\n* Con el pipeline, preprocesamos los datos de entrenamiento y ajustamos el modelo en una sola línea de código. (Por el contrario, sin una canalización, tenemos que realizar la imputación, la codificación one-hot y el entrenamiento del modelo en pasos separados. ¡Esto se vuelve especialmente complicado si tenemos que lidiar con variables numéricas y categóricas!)\n\n* Con la canalización, suministramos las características no procesadas en X_valid al comando predict() y la canalización preprocesa automáticamente las características antes de generar predicciones. (Sin embargo, sin una canalización, debemos recordar preprocesar los datos de validación antes de hacer predicciones).","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:57:37.072848Z","iopub.execute_input":"2024-03-21T17:57:37.073517Z","iopub.status.idle":"2024-03-21T17:57:45.824332Z","shell.execute_reply.started":"2024-03-21T17:57:37.073469Z","shell.execute_reply":"2024-03-21T17:57:45.822539Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"MAE: 160679.18917034855\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Conclusión\n\nLas canalizaciones son valiosas para limpiar el código de aprendizaje automático y evitar errores, y son especialmente útiles para flujos de trabajo con preprocesamiento de datos sofisticado.","metadata":{}}]}