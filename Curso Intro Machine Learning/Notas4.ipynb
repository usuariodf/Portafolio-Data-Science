{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":38454,"sourceType":"datasetVersion","datasetId":2709}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introducción\nLos árboles de decisión te dejan con una decisión difícil. Un árbol profundo con muchas hojas se ajustará demasiado porque cada predicción proviene de datos históricos de sólo las pocas casas en su hoja. Pero un árbol poco profundo y con pocas hojas tendrá un mal desempeño porque no logra capturar tantas distinciones en los datos sin procesar.\n\nIncluso las técnicas de modelado más sofisticadas de hoy enfrentan esta tensión entre el desajuste y el sobreajuste. Sin embargo, muchos modelos tienen ideas inteligentes que pueden conducir a un mejor rendimiento. Veremos el Random Forests como ejemplo.\n\nEl Random Forests utiliza muchos árboles y realiza una predicción promediando las predicciones de cada árbol componente. Generalmente tiene una precisión predictiva mucho mejor que un árbol de decisión único y funciona bien con parámetros predeterminados. Si continúa modelando, podrá aprender más modelos con un rendimiento aún mejor, pero muchos de ellos son sensibles a la hora de obtener los parámetros correctos.","metadata":{}},{"cell_type":"markdown","source":"### Ejemplo\nYa has visto el código para cargar los datos varias veces. Al final de la carga de datos, tenemos las siguientes variables:\n\n* tren_X\n* val_X\n* tren_y\n* val_y\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n    \n# Load data\nmelbourne_file_path = '/kaggle/input/melbourne-housing-snapshot/melb_data.csv'\nmelbourne_data = pd.read_csv(melbourne_file_path) \n# Filter rows with missing values\nmelbourne_data = melbourne_data.dropna(axis=0)\n# Choose target and features\ny = melbourne_data.Price\nmelbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n                        'YearBuilt', 'Lattitude', 'Longtitude']\nX = melbourne_data[melbourne_features]\n\nfrom sklearn.model_selection import train_test_split\n\n# split data into training and validation data, for both features and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T13:34:52.004070Z","iopub.execute_input":"2024-01-21T13:34:52.004718Z","iopub.status.idle":"2024-01-21T13:34:54.225675Z","shell.execute_reply.started":"2024-01-21T13:34:52.004676Z","shell.execute_reply":"2024-01-21T13:34:54.224453Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Construimos un modelo de Random Forests  de manera similar a cómo construimos un árbol de decisión en scikit-learn, esta vez usando la clase RandomForestRegressor en lugar de DecisionTreeRegressor.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor(random_state=3)\nforest_model.fit(train_X, train_y)\nmelb_preds = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, melb_preds))","metadata":{"execution":{"iopub.status.busy":"2024-01-21T13:37:22.147204Z","iopub.execute_input":"2024-01-21T13:37:22.147579Z","iopub.status.idle":"2024-01-21T13:37:23.981159Z","shell.execute_reply.started":"2024-01-21T13:37:22.147555Z","shell.execute_reply":"2024-01-21T13:37:23.979882Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"190949.01827661472\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Conclusión\n\nEs probable que haya margen de mejora adicional, pero se trata de una gran mejora con respecto al error del mejor árbol de decisión de 250.000. Hay parámetros que le permiten cambiar el rendimiento del bosque aleatorio de la misma manera que cambiamos la profundidad máxima del árbol de decisión único. Pero una de las mejores características de los modelos Random Forest es que generalmente funcionan razonablemente incluso sin este ajuste.","metadata":{}}]}